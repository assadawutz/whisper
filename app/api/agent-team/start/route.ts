import { NextRequest, NextResponse } from "next/server";
import { nanoid } from "nanoid";

// Types
type LLMProvider = "gemini" | "ollama" | "openai" | "codex";

const AGENT_REGISTRY: Record<
  string,
  { name: string; role: string; mission: string; principles: string[] }
> = {
  miralyn: {
    name: "Miralyn",
    role: "Architect",
    mission: "‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡∏ß‡πâ‡∏≤‡∏á ‡∏°‡∏≠‡∏á Dependency ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡πÅ‡∏•‡∏∞‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏¢‡∏±‡πà‡∏á‡∏¢‡∏∑‡∏ô",
    principles: ["holistic-design", "dependency-aware", "future-proof"],
  },
  penna: {
    name: "Penna",
    role: "Coder",
    mission: "‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô TypeScript ‡πÅ‡∏•‡∏∞ Tailwind CSS ‡πÉ‡∏´‡πâ‡πÄ‡∏ô‡∏µ‡πâ‡∏¢‡∏ö‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î 1:1 ‡∏ï‡∏≤‡∏°‡πÅ‡∏ö‡∏ö",
    principles: ["pixel-perfect", "clean-code", "type-safety"],
  },
  safetia: {
    name: "Safetia",
    role: "Security",
    mission: "‡∏Ñ‡∏∏‡∏°‡∏Å‡∏é‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ ‡∏´‡πâ‡∏≤‡∏° Hardcode, ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ .env",
    principles: ["defensive-first", "no-secrets-in-code", "safe-by-default"],
  },
  flux: {
    name: "Flux",
    role: "Weaver",
    mission: "‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ State ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå: snapshot + rollback ‡πÑ‡∏î‡πâ‡πÄ‡∏£‡πá‡∏ß",
    principles: ["atomic-writes", "rollback-ready", "preview-first"],
  },
  checkka: {
    name: "Checkka",
    role: "Runner",
    mission: "‡∏£‡∏±‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Error ‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡πá‡∏ö Evidence ‡∏à‡∏≤‡∏Å Terminal",
    principles: ["evidence-based", "thorough-check", "accurate-reporting"],
  },
};

// In-memory storage
const tasks = new Map<string, any>();
const memories = new Map<string, any[]>();
const scars = new Map<string, any[]>();

/**
 * Call LLM based on provider
 */
async function callLLM(
  prompt: string,
  config: {
    provider: LLMProvider;
    apiKey?: string;
    model?: string;
    ollamaUrl?: string;
  },
): Promise<string> {
  const { provider, apiKey, model, ollamaUrl } = config;

  if (provider === "ollama") {
    // Ollama (Local)
    const baseUrl = ollamaUrl || "http://localhost:11434";
    const res = await fetch(`${baseUrl}/api/chat`, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        model: model || "llama3.2",
        messages: [{ role: "user", content: prompt }],
        stream: false,
      }),
    });

    if (!res.ok) throw new Error(`Ollama error: ${res.status}`);
    const data = await res.json();
    return data.message?.content || "";
  }

  if (provider === "openai" || provider === "codex") {
    // OpenAI / Codex
    const res = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${apiKey}`,
      },
      body: JSON.stringify({
        model: model || (provider === "codex" ? "gpt-4o" : "gpt-4o-mini"),
        messages: [{ role: "user", content: prompt }],
        temperature: 0.7,
      }),
    });

    if (!res.ok) throw new Error(`OpenAI error: ${res.status}`);
    const data = await res.json();
    return data.choices?.[0]?.message?.content || "";
  }

  // Gemini (default)
  const url = `https://generativelanguage.googleapis.com/v1beta/models/${model || "gemini-2.0-flash"}:generateContent?key=${encodeURIComponent(apiKey || "")}`;
  const res = await fetch(url, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      contents: [{ parts: [{ text: prompt }] }],
      generationConfig: { temperature: 0.7 },
    }),
  });

  if (!res.ok) throw new Error(`Gemini error: ${res.status}`);
  const data = await res.json();
  return data.candidates?.[0]?.content?.parts?.[0]?.text || "";
}

export async function POST(req: NextRequest) {
  try {
    const {
      goal,
      apiKey,
      provider = "gemini",
      model,
      ollamaUrl,
    } = await req.json();

    if (!goal) {
      return NextResponse.json({ error: "Missing goal" }, { status: 400 });
    }

    // Validate based on provider
    if (provider !== "ollama" && !apiKey) {
      return NextResponse.json(
        { error: "Missing apiKey for cloud provider" },
        { status: 400 },
      );
    }

    const taskId = nanoid();
    const startAgent = "miralyn";
    const agent = AGENT_REGISTRY[startAgent];

    const task = {
      id: taskId,
      goal,
      status: "in_progress",
      currentAgent: startAgent,
      history: [],
      startedAt: Date.now(),
      provider,
      model,
    };

    // Build memory context
    const agentMemories = memories.get(startAgent) || [];
    const agentScars = scars.get(startAgent) || [];

    let memoryContext = "";

    if (agentMemories.length > 0) {
      const recentSuccesses = agentMemories
        .filter((m) => m.outcome === "success")
        .slice(-3);
      if (recentSuccesses.length > 0) {
        memoryContext += "\n\n## üß† PAST LEARNINGS\n";
        recentSuccesses.forEach((m) => {
          memoryContext += `- ‚úÖ ${m.taskType}: ${m.learning}\n`;
        });
      }
    }

    if (agentScars.length > 0) {
      memoryContext += "\n\n## ‚ö†Ô∏è SCARS (‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î)\n";
      agentScars.slice(-3).forEach((s) => {
        memoryContext += `- ‚ùå ${s.error} ‚Üí ‚úÖ ${s.fix}\n`;
      });
    }

    const fullPrompt = `# ü§ñ ${agent.name} - ${agent.role}

## Mission
${agent.mission}

## Principles
${agent.principles.map((p) => `- ${p}`).join("\n")}
${memoryContext}

## üîÑ SELF-LEARNING MODE: ACTIVE
‡∏à‡∏≥‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏à‡∏≤‡∏Å‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥ ‡πÅ‡∏•‡∏∞‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏Å‡∏¥‡∏î

## Team Members
- **Miralyn** (Architect), **Penna** (Coder), **Safetia** (Security), **Flux** (Weaver), **Checkka** (Runner)

‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ handoff: ## üîÄ HANDOFF ‚Üí ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡πÉ‡∏´‡πâ **[‡∏ä‡∏∑‡πà‡∏≠ Agent]**

---

USER: ‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏°‡∏≠‡∏ö‡∏´‡∏°‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà:
"${goal}"

‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô:

## üìã ANALYSIS
## üéØ PLAN
## üîÄ HANDOFF (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
## üìö LEARNING`;

    // Call LLM with selected provider
    const response = await callLLM(fullPrompt, {
      provider: provider as LLMProvider,
      apiKey,
      model,
      ollamaUrl,
    });

    // Add to history
    task.history.push({
      id: nanoid(),
      ts: Date.now(),
      from: "system",
      to: startAgent,
      content: `‡∏á‡∏≤‡∏ô: ${goal}`,
      type: "task",
    });

    task.history.push({
      id: nanoid(),
      ts: Date.now(),
      from: startAgent,
      to: "user",
      content: response,
      type: "result",
    });

    // Store learning
    const existingMemories = memories.get(startAgent) || [];
    existingMemories.push({
      id: nanoid(),
      ts: Date.now(),
      taskType: goal.slice(0, 50),
      outcome: "success",
      learning: `Analyzed using ${provider}/${model || "default"}`,
    });
    memories.set(startAgent, existingMemories.slice(-50));

    // Check handoff
    const handoffMatch = response.match(
      /##\s*üîÄ\s*HANDOFF[\s\S]*?(?:‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡πÉ‡∏´‡πâ|‚Üí)\s*\*?\*?(\w+)\*?\*?/i,
    );
    if (handoffMatch) {
      const targetAgent = handoffMatch[1].toLowerCase();
      if (AGENT_REGISTRY[targetAgent]) {
        task.currentAgent = targetAgent;
      }
    }

    tasks.set(taskId, task);

    return NextResponse.json(task);
  } catch (err: any) {
    console.error("Agent Team Error:", err);
    return NextResponse.json({ error: err.message }, { status: 500 });
  }
}

export async function GET() {
  const stats: Record<string, any> = {};

  for (const [agentId, agentMemories] of memories.entries()) {
    stats[agentId] = {
      totalMemories: agentMemories.length,
      successRate: Math.round(
        (agentMemories.filter((m) => m.outcome === "success").length /
          (agentMemories.length || 1)) *
          100,
      ),
      scarsCount: (scars.get(agentId) || []).length,
    };
  }

  return NextResponse.json({
    stats,
    tasks: Array.from(tasks.values()),
    providers: ["gemini", "ollama", "openai", "codex"],
  });
}
